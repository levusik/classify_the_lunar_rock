{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet 18 Pytorch\n",
    "Before training launched two external scripts : dataset_explore.py to check for sure if the data is correct and validation_split.py to create validation data. <br>\n",
    "I took 15 % of all data ( 1800 images overall ). Script took random images from training data directory with the same distribution of data ( 50%-50%, same as training data).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA\n"
     ]
    }
   ],
   "source": [
    "# utils \n",
    "import numpy as np \n",
    "\n",
    "# interacting with os \n",
    "import os, sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# pytorch dependencies\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch import nn\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Checking if GPU is available\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"using CUDA\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy/paste from ImageFolder class pytorch ( just added to return path argument )\n",
    "class DatasetWithLabels(datasets.ImageFolder):\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dir path : '.\\Train Images' : content '['Large', 'Small', 'valid_lock']'\n",
      "Validation dir path : '.\\Validation Images' : content '['Large', 'Small']'\n",
      "Test dir path : '.\\TEST' : content '['Test Images']'\n"
     ]
    }
   ],
   "source": [
    "data_dir = '.'\n",
    "TRAIN = 'Train Images'\n",
    "VAL = 'Validation Images'\n",
    "TEST = 'TEST'\n",
    "\n",
    "def p(label, path):\n",
    "    print(\"{} dir path : '{}' : content '{}'\".format(label, path, os.listdir(path)))\n",
    "\n",
    "p(\"Train\", os.path.join(data_dir, TRAIN))\n",
    "p(\"Validation\", os.path.join(data_dir, VAL))\n",
    "p(\"Test\", os.path.join(data_dir, TEST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10198 images under Train Images\n",
      "Loaded 1800 images under Validation Images\n",
      "Loaded 7534 images under TEST\n",
      "Classes: \n",
      "['Large', 'Small']\n"
     ]
    }
   ],
   "source": [
    "# rescale images \n",
    "IMG_WIDTH = 360\n",
    "IMG_HEIGHT = 240\n",
    "\n",
    "# data transformations ( just rescale and convert to tensor)\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    VAL: transforms.Compose([\n",
    "        transforms.Resize( (IMG_HEIGHT, IMG_WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    TEST : transforms.Compose([\n",
    "        transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "}\n",
    "\n",
    "# image datasets\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(data_dir, x), \n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=8,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [TRAIN, VAL]\n",
    "}\n",
    "\n",
    "dataloaders[TEST] = torch.utils.data.DataLoader( image_datasets[TEST], batch_size=8,shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n",
    "\n",
    "for x in [TRAIN, VAL, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "    \n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[TRAIN].classes\n",
    "print(image_datasets[TRAIN].classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(inp)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "def show_databatch(inputs, classes):\n",
    "    out = torchvision.utils.make_grid(inputs)\n",
    "    imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "inputs, classes = next(iter(dataloaders[TRAIN]))\n",
    "show_databatch(inputs, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# training the architecture \n",
    "Resnet 18 architecure gave best accuracies ( compared to vgg11 and vgg13 with natch normalizations). Classificator Layer is replaced with 2 linear layer, dropout with large value ( 0.5 ), and relu activation ( no activation at the end of classifier, because loss applies softmax value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetuning = models.resnet18(pretrained=True)\n",
    "\n",
    "in_features = model_finetuning.fc.in_features # 512 * 7 * 7\n",
    "x = nn.Sequential(nn.Linear(in_features, 128), \n",
    "                   nn.ReLU(),\n",
    "                   nn.Dropout(0.5), \n",
    "                   nn.Linear(128, 2))\n",
    "\n",
    "\n",
    "model_finetuning.fc = x\n",
    "\n",
    "model_finetuning = model_finetuning.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.Adam(model_finetuning.parameters())\n",
    "\n",
    "# gamma = decaying factor\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# General function that trains pytorch model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
    "        print(\"*\"*16)\n",
    "        \n",
    "        # each epoch has training and validation phase\n",
    "        for phase in [TRAIN, VAL]:\n",
    "            if phase ==TRAIN:\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward \n",
    "                with torch.set_grad_enabled(phase == TRAIN):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                    if phase == TRAIN:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == TRAIN and scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc  = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == VAL and epoch_acc >= best_acc:\n",
    "                print(\"Swapping {} -> {}\".format(best_acc, epoch_acc))\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "****************\n",
      "Train Images Loss: 0.0918 Acc: 0.9751\n",
      "Validation Images Loss: 0.0188 Acc: 0.9939\n",
      "Swapping 0.0 -> 0.9938888888888889\n",
      "Epoch 1/9\n",
      "****************\n",
      "Train Images Loss: 0.0239 Acc: 0.9928\n",
      "Validation Images Loss: 0.0123 Acc: 0.9967\n",
      "Swapping 0.9938888888888889 -> 0.9966666666666667\n",
      "Epoch 2/9\n",
      "****************\n",
      "Train Images Loss: 0.0226 Acc: 0.9943\n",
      "Validation Images Loss: 0.0040 Acc: 0.9989\n",
      "Swapping 0.9966666666666667 -> 0.9988888888888889\n",
      "Epoch 3/9\n",
      "****************\n",
      "Train Images Loss: 0.0247 Acc: 0.9942\n",
      "Validation Images Loss: 0.0999 Acc: 0.9789\n",
      "Epoch 4/9\n",
      "****************\n",
      "Train Images Loss: 0.0215 Acc: 0.9941\n",
      "Validation Images Loss: 0.0038 Acc: 0.9994\n",
      "Swapping 0.9988888888888889 -> 0.9994444444444445\n",
      "Epoch 5/9\n",
      "****************\n",
      "Train Images Loss: 0.0166 Acc: 0.9947\n",
      "Validation Images Loss: 0.0085 Acc: 0.9978\n",
      "Epoch 6/9\n",
      "****************\n",
      "Train Images Loss: 0.0082 Acc: 0.9969\n",
      "Validation Images Loss: 0.0007 Acc: 1.0000\n",
      "Swapping 0.9994444444444445 -> 1.0\n",
      "Epoch 7/9\n",
      "****************\n",
      "Train Images Loss: 0.0080 Acc: 0.9976\n",
      "Validation Images Loss: 0.0029 Acc: 0.9994\n",
      "Epoch 8/9\n",
      "****************\n",
      "Train Images Loss: 0.0039 Acc: 0.9986\n",
      "Validation Images Loss: 0.0007 Acc: 1.0000\n",
      "Swapping 1.0 -> 1.0\n",
      "Epoch 9/9\n",
      "****************\n",
      "Train Images Loss: 0.0024 Acc: 0.9994\n",
      "Validation Images Loss: 0.0005 Acc: 1.0000\n",
      "Swapping 1.0 -> 1.0\n",
      "Training complete in 13m 18s\n",
      "Best val Acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_finetuning, criterion, optimizer_ft, scheduler=scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 %\n",
      "5 %\n",
      "10 %\n",
      "15 %\n",
      "20 %\n",
      "25 %\n",
      "30 %\n",
      "35 %\n",
      "40 %\n",
      "45 %\n",
      "50 %\n",
      "55 %\n",
      "60 %\n",
      "65 %\n",
      "70 %\n",
      "75 %\n",
      "80 %\n",
      "85 %\n",
      "90 %\n",
      "95 %\n",
      "100 %\n"
     ]
    }
   ],
   "source": [
    "test = DatasetWithLabels(os.path.join(data_dir, TEST), transform=data_transforms[TEST])\n",
    "test_dataloader = torch.utils.data.DataLoader( test, batch_size=8,shuffle=False, num_workers=0)\n",
    "\n",
    "vs= {\"1\" : \"Small\", \"0\" : \"Large\"}\n",
    "cnt = 0\n",
    "curr  = 0\n",
    "perct = 5\n",
    "\n",
    "f = open(\"submission.csv\", \"w\")\n",
    "f.write(\"Image_File,Class\\n\")\n",
    "for inputs, labels, fns in test_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model_ft(inputs)\n",
    "    _, pds = torch.max(outputs, 1)\n",
    "    \n",
    "    pds = pds.cpu().numpy().reshape(-1, 1)\n",
    "    fns = np.array([file.split(\"\\\\\")[-1] for file in fns]).reshape(-1, 1)\n",
    "    for x, y in np.concatenate([fns, pds], axis=1):\n",
    "        f.write(x + \",\" + vs[y] + \"\\n\")\n",
    "    \n",
    "    cnt += len(fns)\n",
    "    if cnt / len(test) * 100 >= curr:\n",
    "        curr += perct\n",
    "        print(round(cnt / len(test) * 100), \"%\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
